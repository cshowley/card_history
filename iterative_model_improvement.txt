research ideas for model improvement

monotone_constraints
- attempts to implement this on 01/13 did not work. Implemented monotonic post-processing instead. Would be good to figure this out for the model training step though

feature reduction:

1. brute force
currently lowest 5% of features as determined by xgboost feature importance are dropped
given fixed hyperparameters, run training w/ validation+test sets on greater proportions of features not included.

e.g.
drop lowest 5% of features
lowest 10% of features
lowest 15% of features
...
lowest 95% of features

2. intelligent selection
plot a cumulative sum of total feature importance on y-axis, n features on x-axis
compute elbow point of plot and drop all features below that (i.e. point of diminishing returns per feature)

--

new/modified features:
Use the following hardcoded features as additional hyperparameters\
NUMBER_OF_BIDS_FILTER = 3 -> range(1,10)
TOP_N_SELLERS = 3 -> range(3,20)
N_SALES_BACK = 5 -> range(2,20)
WEEKS_BACK_LIST = [1,2,3,4] -> range(1,26)

include historical price action of related cards (not ready yet)


